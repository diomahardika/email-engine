# Menerima data dari Filebeat di port 5044
input {
  beats {
    port => 5044
  }
}

# Filter sekarang jauh lebih sederhana dan fokus pada pembersihan data (data enrichment & cleanup)
filter {
  # 1. Atur Timestamp Utama
  # Menggunakan field 'datetime' dari log asli sebagai @timestamp.
  # Ini memastikan waktu yang tercatat adalah waktu kejadian, bukan waktu data diproses.
  date {
    match => [ "datetime", "ISO8601" ]
    target => "@timestamp"
  }

  # 2. Bersihkan Field yang Tidak Perlu
  # Menghapus field-field yang redundan atau memakan banyak tempat setelah diproses.
  mutate {
    remove_field => [
      "message",          # Isinya hanya "Log email dibuat", sudah tidak relevan.
      "event",            # Field ini, terutama 'event.original', sangat besar dan duplikat.
      "datetime",         # Nilainya sudah dipindahkan ke @timestamp.
      "ecs",              # Skema ECS, bisa dihapus jika tidak digunakan untuk korelasi.
      "agent",            # Metadata tentang agen Filebeat, bisa dihapus untuk menghemat ruang.
      "host",             # Informasi host juga bagian dari metadata agent.
      "log",              # Informasi tentang file sumber, seringkali tidak diperlukan di ES.
      "input"             # Informasi tipe input filebeat.
    ]
  }

  # 3. Penanganan Error Parsing dari Filebeat
  # Jika Filebeat gagal mem-parsing JSON, field [error][message] akan ada.
  # Kita beri tag agar mudah dicari dan dianalisis di Kibana.
  if [error][message] {
    mutate {
      add_tag => ["_filebeat_json_parse_failure"]
    }
  }
}

# Mengirim data yang sudah bersih ke Elasticsearch
output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    # Pola indeks harian sudah sangat baik, tidak perlu diubah.
    index => "email-engine-logs-%{+YYYY.MM.dd}"
  }
}